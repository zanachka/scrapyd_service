[cluster]
cluster_name = cluster
node_name = master
identity = master
slave_hosts = ["127.0.0.1:6801"]
is_all_code_same = true
pull_code_by_git = true
local_crawler_code_path = E:\AllWorkCode\datacrawl
branch = develop

[projects]
regex_trainer = regex_trainer
newspapers = newspapers

[scrapyd]
bind_address = 127.0.0.1
http_port   = 6800
username    =
password    =
logs_dir    = logs
dbs_dir     = dbs
items_dir   =
jobs_to_keep = 5
max_proc    = 0
max_proc_per_cpu = 4
finished_to_keep = 100
poll_interval = 5.0
ping_interval = 5.0
debug       = off
runner      = scrapyd_service.runner
application = scrapyd_service.app.application
launcher    = scrapyd_service.launcher.Launcher
webroot     = scrapyd_service.website.Root

[master-services]
schedule.json     = scrapyd_service.webservices_master.Schedule
cancel.json       = scrapyd_service.webservices_master.Cancel
listprojects.json = scrapyd_service.webservices_master.ListProjects
listspiders.json  = scrapyd_service.webservices_master.ListSpiders
delproject.json   = scrapyd_service.webservices_master.DeleteProject
listjobs.json     = scrapyd_service.webservices_master.ListJobs
daemonstatus.json = scrapyd_service.webservices_master.DaemonStatus
pullcode.json = scrapyd_service.webservices_master.PullCode
pushcode.json = scrapyd_service.webservices_master.PushCode
crawllog.json = scrapyd_service.webservices_master.CrawlLog


[slave-services]
schedule.json     = scrapyd_service.webservices_slave.Schedule
cancel.json       = scrapyd_service.webservices_slave.Cancel
listprojects.json = scrapyd_service.webservices_slave.ListProjects
listspiders.json  = scrapyd_service.webservices_slave.ListSpiders
delproject.json   = scrapyd_service.webservices_slave.DeleteProject
listjobs.json     = scrapyd_service.webservices_slave.ListJobs
daemonstatus.json = scrapyd_service.webservices_slave.DaemonStatus
pullcode.json = scrapyd_service.webservices_slave.PullCode
crawllog.json = scrapyd_service.webservices_slave.CrawlLog
